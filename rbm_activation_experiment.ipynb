{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8581a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Shape of data: (8, 100)\n",
      "INFO:root:Shape of noisy data: (8, 100)\n",
      "INFO:root:Testing activation function: sigmoid\n",
      "INFO:root:Epoch 100/1000, Reconstruction Error: 0.0008, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 200/1000, Reconstruction Error: 0.0002, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 300/1000, Reconstruction Error: 0.0002, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 400/1000, Reconstruction Error: 0.0001, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 500/1000, Reconstruction Error: 0.0001, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 600/1000, Reconstruction Error: 0.0001, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 700/1000, Reconstruction Error: 0.0001, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 800/1000, Reconstruction Error: 0.0001, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 900/1000, Reconstruction Error: 0.0001, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 1000/1000, Reconstruction Error: 0.0001, Elapsed Time: 0.0002\n",
      "INFO:root:Average Error: 0.0025350922250412405 Average Epoch Time: 0.00018507933616638184\n",
      "INFO:root:Testing activation function: relu\n",
      "INFO:root:Epoch 100/1000, Reconstruction Error: 0.0755, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 200/1000, Reconstruction Error: 0.0743, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 300/1000, Reconstruction Error: 0.0811, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 400/1000, Reconstruction Error: 0.0809, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 500/1000, Reconstruction Error: 0.0807, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 600/1000, Reconstruction Error: 0.0807, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 700/1000, Reconstruction Error: 0.0801, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 800/1000, Reconstruction Error: 0.0801, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 900/1000, Reconstruction Error: 0.0800, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 1000/1000, Reconstruction Error: 0.0800, Elapsed Time: 0.0002\n",
      "INFO:root:Average Error: 0.08131056279633202 Average Epoch Time: 0.00016807103157043458\n",
      "INFO:root:Testing activation function: leaky_relu\n",
      "INFO:root:Epoch 100/1000, Reconstruction Error: 0.1748, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 200/1000, Reconstruction Error: 0.2019, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 300/1000, Reconstruction Error: 0.2048, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 400/1000, Reconstruction Error: 0.1970, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 500/1000, Reconstruction Error: 0.1958, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 600/1000, Reconstruction Error: 0.1922, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 700/1000, Reconstruction Error: 0.1938, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 800/1000, Reconstruction Error: 0.1940, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 900/1000, Reconstruction Error: 0.1944, Elapsed Time: 0.0002\n",
      "INFO:root:Epoch 1000/1000, Reconstruction Error: 0.1944, Elapsed Time: 0.0002\n",
      "INFO:root:Average Error: 0.1938750775900841 Average Epoch Time: 0.00019257593154907226\n",
      "INFO:root:Activation Function Results:\n",
      "INFO:root:Activation      Reconstruction Error Accuracy (%)   \n",
      "INFO:root:sigmoid         0.2263               87.50          \n",
      "INFO:root:relu            0.1537               87.50          \n",
      "INFO:root:leaky_relu      0.2575               75.00          \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"\")\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "\n",
    "class RestrictedBoltzmannMachine:\n",
    "    def __init__(self, n_visible, n_hidden, activation, learning_rate=0.1, n_epochs=1000, batch_size=10, decay_rate=0.99):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.decay_rate = decay_rate\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.uniform(-0.1, 0.1, (n_visible, n_hidden))\n",
    "        self.visible_bias = np.zeros(n_visible)\n",
    "        self.hidden_bias = np.zeros(n_hidden)\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Apply the selected activation function.\"\"\"\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"relu\":\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            return np.where(x > 0, x, 0.01 * x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "    def sample_probabilities(self, probs):\n",
    "        \"\"\"Sample binary states based on probabilities.\"\"\"\n",
    "        return (np.random.rand(*probs.shape) < probs).astype(np.float32)\n",
    "\n",
    "    def contrastive_divergence(self, data):\n",
    "        \"\"\"Perform one step of contrastive divergence.\"\"\"\n",
    "        # Positive phase\n",
    "        pos_hidden_probs = self.activate(np.dot(data, self.weights) + self.hidden_bias)\n",
    "        pos_hidden_states = self.sample_probabilities(pos_hidden_probs)\n",
    "        pos_associations = np.dot(data.T, pos_hidden_probs)\n",
    "\n",
    "        # Negative phase\n",
    "        neg_visible_probs = self.activate(np.dot(pos_hidden_states, self.weights.T) + self.visible_bias)\n",
    "        neg_hidden_probs = self.activate(np.dot(neg_visible_probs, self.weights) + self.hidden_bias)\n",
    "        neg_associations = np.dot(neg_visible_probs.T, neg_hidden_probs)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights += self.learning_rate * (pos_associations - neg_associations) / data.shape[0]\n",
    "        self.visible_bias += self.learning_rate * np.mean(data - neg_visible_probs, axis=0)\n",
    "        self.hidden_bias += self.learning_rate * np.mean(pos_hidden_probs - neg_hidden_probs, axis=0)\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"Train the RBM using the provided data.\"\"\"\n",
    "        total_times = []\n",
    "        errors = []\n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(data)\n",
    "            start_time = time.time()\n",
    "            for i in range(0, data.shape[0], self.batch_size):\n",
    "                batch = data[i:i + self.batch_size]\n",
    "                self.contrastive_divergence(batch)\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            error = np.mean((data - self.reconstruct(data)) ** 2)\n",
    "\n",
    "            total_times.append(elapsed_time)\n",
    "            errors.append(error)\n",
    "\n",
    "            # Apply learning rate decay\n",
    "            self.learning_rate *= self.decay_rate\n",
    "\n",
    "            # Calculate reconstruction error\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                logger.info(f\"Epoch {epoch + 1}/{self.n_epochs}, Reconstruction Error: {error:.4f}, Elapsed Time: {elapsed_time:.4f}\")\n",
    "\n",
    "        logger.info(f\"Average Error: {np.mean(errors)} Average Epoch Time: {np.mean(total_times)}\")\n",
    "\n",
    "    def reconstruct(self, data):\n",
    "        \"\"\"Reconstruct visible units from hidden units.\"\"\"\n",
    "        hidden_probs = self.activate(np.dot(data, self.weights) + self.hidden_bias)\n",
    "        visible_probs = self.activate(np.dot(hidden_probs, self.weights.T) + self.visible_bias)\n",
    "        return visible_probs\n",
    "\n",
    "    def visualize_weights(self):\n",
    "        \"\"\"Visualize weights as a heatmap.\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(self.weights, cmap='viridis', aspect='auto')\n",
    "        plt.colorbar(label=\"Weight Magnitude\")\n",
    "        plt.title(\"Weight Heatmap\")\n",
    "        plt.xlabel(\"Hidden Units\")\n",
    "        plt.ylabel(\"Visible Units\")\n",
    "        plt.show()\n",
    "\n",
    "def generate_numerals():\n",
    "    \"\"\"Generate 10x10 binary arrays representing the digits 0-7.\"\"\"\n",
    "    numerals = [\n",
    "        # Digit 0\n",
    "        np.array([\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 1\n",
    "        np.array([\n",
    "            [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 2\n",
    "        np.array([\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 3\n",
    "        np.array([\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 4\n",
    "        np.array([\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 5\n",
    "        np.array([\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 6\n",
    "        np.array([\n",
    "            [0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 7\n",
    "        np.array([\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "    ]\n",
    "\n",
    "    # Flatten each 10x10 array into a 1D array of size 100\n",
    "    flattened_numerals = [numeral.flatten() for numeral in numerals]\n",
    "    return np.array(flattened_numerals)\n",
    "\n",
    "def add_custom_noise(data, noise_level=0.2):\n",
    "    noisy_data = data.copy()\n",
    "    noise = np.random.binomial(1, noise_level, data.shape)\n",
    "    noisy_data = np.abs(noisy_data - noise)  # Flip bits based on noise\n",
    "    return noisy_data\n",
    "\n",
    "# Function to binarize the reconstructed data to ensure black and white output\n",
    "def binarize_data(data, threshold=0.5):\n",
    "    \"\"\"Convert probabilities to binary values (0 or 1) based on a threshold.\"\"\"\n",
    "    return (data >= threshold).astype(np.float32)\n",
    "\n",
    "# Function to calculate reconstruction error\n",
    "def calculate_reconstruction_error(original, reconstructed):\n",
    "    \"\"\"Calculate the mean squared error between original and reconstructed data.\"\"\"\n",
    "    return np.mean((original - reconstructed) ** 2)\n",
    "\n",
    "# Function to calculate Hamming distance\n",
    "def hamming_distance(a, b):\n",
    "    \"\"\"Calculate the Hamming distance between two binary arrays.\"\"\"\n",
    "    return np.sum(a != b)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--n_visible', type=int, default=100)\n",
    "    parser.add_argument('--n_hidden', type=int, default=150)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--n_epochs', type=int, default=1000)\n",
    "    parser.add_argument('--batch_size', type=int, default=4)\n",
    "    parser.add_argument('-o', '--output', default=None, type=str, help=\"Output for the metrics\")\n",
    "    parser.add_argument('--f', type=str, help=\"Kernel file (ignored in Jupyter)\", default=None)  # Added to handle Jupyter arguments\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    if opts.output is not None:\n",
    "        fileHandler = logging.FileHandler(opts.output)\n",
    "        fileHandler.setFormatter(formatter)\n",
    "        logger.addHandler(fileHandler)\n",
    "\n",
    "    data = generate_numerals()\n",
    "    logger.info(f\"Shape of data: {data.shape}\")  # Should print (8, 100)\n",
    "\n",
    "    noisy_data = add_custom_noise(data, noise_level=0.01)  # Reduced noise level\n",
    "    logger.info(f\"Shape of noisy data: {noisy_data.shape}\")  # Should also be (8, 100)\n",
    "\n",
    "    activations = [\"sigmoid\", \"relu\", \"leaky_relu\"]\n",
    "    results = []\n",
    "\n",
    "    for activation in activations:\n",
    "        logger.info(f\"Testing activation function: {activation}\")\n",
    "        rbm = RestrictedBoltzmannMachine(\n",
    "            n_visible=opts.n_visible,\n",
    "            n_hidden=opts.n_hidden,\n",
    "            learning_rate=opts.learning_rate,\n",
    "            n_epochs=opts.n_epochs,\n",
    "            batch_size=opts.batch_size,\n",
    "            activation=activation\n",
    "        )\n",
    "        rbm.train(noisy_data)\n",
    "        reconstructed_data = rbm.reconstruct(noisy_data)\n",
    "        reconstructed_data = binarize_data(reconstructed_data)\n",
    "\n",
    "        # Calculate reconstruction error\n",
    "        reconstruction_error = calculate_reconstruction_error(data, reconstructed_data)\n",
    "\n",
    "        # Evaluate accuracy using Hamming distance\n",
    "        threshold = 30\n",
    "        correct_reconstructions = 0\n",
    "        for i in range(len(data)):\n",
    "            distance = hamming_distance(data[i], reconstructed_data[i])\n",
    "            if distance <= threshold:\n",
    "                correct_reconstructions += 1\n",
    "\n",
    "        accuracy = correct_reconstructions / len(data) * 100\n",
    "        results.append((activation, reconstruction_error, accuracy))\n",
    "\n",
    "    # Display results\n",
    "    logger.info(\"Activation Function Results:\")\n",
    "    logger.info(f\"{'Activation':<15} {'Reconstruction Error':<20} {'Accuracy (%)':<15}\")\n",
    "    for activation, error, acc in results:\n",
    "        logger.info(f\"{activation:<15} {error:<20.4f} {acc:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270bdbb6",
   "metadata": {},
   "source": [
    "### Overview of the Experiment\n",
    "\n",
    "This experiment involves the design and training of a **Restricted Boltzmann Machine (RBM)**, a generative stochastic neural network that learns a probability distribution over its input data. The RBM is trained to reconstruct binary data representing numerals (digits 0-7) while exploring the effects of different activation functions on reconstruction accuracy and error.\n",
    "\n",
    "---\n",
    "\n",
    "### Background Information\n",
    "\n",
    "#### **Restricted Boltzmann Machine (RBM) Design**\n",
    "- RBMs work by maximizing the likelihood function, which requires estimating averages involving exponential terms. This computation is typically expensive, but techniques like Contrastive Divergence (CD) are used to approximate these averages efficiently.\n",
    "- The RBM consists of two layers:\n",
    "    - **Visible Units**: Represent the input data.\n",
    "    - **Hidden Units**: Capture dependencies and features in the data.\n",
    "\n",
    "#### **Weights and Biases**\n",
    "- **Weights**: Represent the connections between visible and hidden units. These are learned during training.\n",
    "- **Visible Bias**: Adjusts the activation of visible units.\n",
    "- **Hidden Bias**: Adjusts the activation of hidden units.\n",
    "\n",
    "---\n",
    "\n",
    "### Activation Functions Used\n",
    "\n",
    "1. **Sigmoid**:\n",
    "     - Suitable for binary data as it outputs values between 0 and 1.\n",
    "     - Helps in reconstructing data that falls within this range.\n",
    "     - May suffer from vanishing gradient issues during training.\n",
    "\n",
    "2. **ReLU (Rectified Linear Unit)**:\n",
    "     - Speeds up training and is preferred for hidden layers.\n",
    "     - Helps mitigate the vanishing gradient problem by allowing gradients to flow for positive inputs.\n",
    "\n",
    "3. **Leaky ReLU**:\n",
    "     - Addresses the \"dying ReLU\" problem by allowing a small gradient for negative inputs.\n",
    "     - Prevents neurons from becoming inactive during training.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Parameters\n",
    "\n",
    "- **Learning Rate**: 0.1\n",
    "- **Number of Epochs**: 1000\n",
    "- **Batch Size**: 10\n",
    "- **Decay Rate**: 0.99 (reduces the learning rate over time to stabilize training)\n",
    "\n",
    "---\n",
    "\n",
    "### Results Summary\n",
    "\n",
    "The experiment evaluated the performance of three activation functions (`sigmoid`, `relu`, `leaky_relu`) based on reconstruction error and accuracy. The results are as follows:\n",
    "\n",
    "| Activation Function | Reconstruction Error | Accuracy (%) |\n",
    "|----------------------|----------------------|--------------|\n",
    "| Sigmoid             | 0.22625             | 87.5         |\n",
    "| ReLU                | 0.15375             | 87.5         |\n",
    "| Leaky ReLU          | 0.2575              | 75.0         |\n",
    "\n",
    "- **ReLU** achieved the lowest reconstruction error and highest accuracy, making it the most effective activation function for this dataset.\n",
    "- **Leaky ReLU** had the highest reconstruction error and lowest accuracy, possibly due to its behavior with binary data.\n",
    "- **Sigmoid** performed well, as expected for binary data, but was slightly outperformed by ReLU. \n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This experiment demonstrates the impact of activation functions on RBM performance. While sigmoid is suitable for binary data, ReLU's ability to mitigate vanishing gradients and speed up training made it the most effective choice for this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
