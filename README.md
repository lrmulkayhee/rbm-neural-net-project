# Restricted Boltzmann Machine (RBM) Neural Network Project

## Overview

This project demonstrates the use of a Restricted Boltzmann Machine (RBM) to reconstruct noisy numeral images. The RBM is trained to denoise and reconstruct visual data, showcasing its ability to learn meaningful latent features.

## Purpose

The purpose of this project is to reconstruct noisy numeral images using an RBM. This approach demonstrates the RBM's ability to denoise and reconstruct visual data effectively, even in the presence of noise.

## Key Results

- **Prediction Accuracy**: Achieved a prediction accuracy of **100%** for noisy numeral images with a noise level of **0.0% to 0.3%**.
- **Reconstruction Error**: The reconstruction error stabilized at **0.3260** during training for a noise level of **0.0**, indicating effective learning.
- **Visual Inspection**: Reconstructed images demonstrated that the RBM successfully recovered key numeral patterns for low noise levels, though performance degraded at higher noise levels.

## Significance

This work highlights the potential of RBMs for image denoising tasks, contributing to neural network research. The results demonstrate the RBM's ability to learn meaningful latent features, making it a valuable tool for preprocessing and enhancing visual data in computer vision systems.

## Problem Description and Network Design

### Problem Statement

The task involves reconstructing noisy numeral images (digits 0-7) represented as 10x10 binary matrices. Each pixel is either active (1) or inactive (0). The primary challenges include:

- **Noise**: Distortions in the input images make it difficult to recover the original numeral patterns.
- **Limited Training Data**: Only 8 exemplars (one for each numeral) are available, requiring the model to generalize effectively.

### Network Design

#### Architecture

The Restricted Boltzmann Machine (RBM) used in this project has:

- **Visible Units**: 100 units, corresponding to the 10x10 pixel grid of the input images.
- **Hidden Units**: 150 units, which capture latent features and patterns in the data.

#### Training Methodology

### Training Methodology

The RBM is trained using the **Contrastive Divergence (CD)** algorithm, which approximates the gradient of the log-likelihood. Key training parameters include:

- **Activation Function**: Sigmoid
- **Number of Epochs**: 1250 epochs to ensure convergence
- **Learning Rate**: 0.1, with no decay applied
- **Batch Size**: 2, allowing efficient updates while leveraging small batches of data
- **Regularization**: L2 regularization with a lambda value of 0.0005

#### Noise Handling and Reconstruction

The RBM learns robust feature representations in the hidden layer. During training, it associates noisy inputs with their corresponding clean outputs. When presented with a noisy image, the RBM reconstructs the original numeral by activating the most likely visible units based on the learned weights and biases.

## Computational Performance Analysis

### Reconstruction Error

The reconstruction error was monitored over **1250 epochs**. The error decreased rapidly during the initial epochs and stabilized at **0.3260** for a noise level of **0.0**, indicating effective learning and convergence.

### Reconstruction Accuracy

### Reconstruction Accuracy

The RBM's accuracy in reconstructing noisy images was evaluated using a **Hamming distance threshold**. The accuracy was **100%** for noise levels up to **0.3%**, meaning the RBM correctly reconstructed all 8 numerals. However, accuracy dropped significantly for higher noise levels.

#### Accuracy Table

| **Noise Level (%)** | **Prediction Accuracy (%)** |
|----------------------|-----------------------------|
| 0.0                  | 100.0                       |
| 0.1                  | 100.0                       |
| 0.2                  | 100.0                       |
| 0.3                  | 100.0                       |
| 0.4                  | 12.5                        |
| 0.5                  | 37.5                        |
| 0.6                  | 37.5                        |
| 0.7                  | 25.0                        |
| 0.8                  | 37.5                        |
| 0.9                  | 12.5                        |

### Visualization

Below are examples of original, noisy, and reconstructed images:

***INSERT PICS HERE***

1. **Original Images**: Clean numeral images without noise.
2. **Noisy Images**: Input images with __________% noise.
3. **Reconstructed Images**: Outputs generated by the RBM.

### Computational Efficiency

## Computational Efficiency

### Parameter Impact on Performance

| **Parameter**       | **Impact on Performance**                                                                 |
|----------------------|------------------------------------------------------------------------------------------|
| **Hidden Units**     | Increasing hidden units improved accuracy but increased training time.                   |
| **Learning Rate**    | Higher rates sped up training but caused instability.                                    |
| **Batch Size**       | Smaller batches improved generalization but slowed training.                            |
| **Noise Level**      | Higher noise levels reduced reconstruction accuracy.                                     |

### Training Time

- The RBM required approximately **0.0004 seconds per epoch**, totaling **~0.5 seconds** for **1250 epochs**.

### Impact of Hyperparameters

- **Hidden Units**: Increasing the number of hidden units improved reconstruction accuracy but increased training time.
- **Epochs**: More epochs led to better convergence, but the reconstruction error stabilized after **1000 epochs**.
- **Learning Rate**: A balanced learning rate ensured faster convergence without instability.
- **Batch Size**: Smaller batch sizes enhanced generalization but required more iterations for training.
- **Noise Level**: Higher noise levels significantly impacted reconstruction accuracy, as shown in the accuracy table.

## Strengths and Weaknesses

## Strengths and Weaknesses

### Strengths

- **Handling Moderate Noise**: The RBM effectively reconstructed images with low noise levels (up to **0.3% noise**).
- **Feature Learning**: The hidden layer captured meaningful latent features, enabling robust reconstructions.
- **Fast Convergence**: The reconstruction error stabilized quickly during training.

### Weaknesses

- **High Noise Levels**: The RBM struggled to reconstruct images with higher noise levels (e.g., **0.4% and above**).
- **Limited Generalization**: With only **8 training exemplars**, the RBM's ability to generalize to unseen noisy inputs is limited.
- **Scalability**: The RBM's performance may degrade when applied to larger images or more complex datasets.

## Future Improvements

### Model Enhancements

- **Deeper Architectures**: Extending the RBM to a Deep Belief Network (DBN) or stacking multiple RBMs could improve its ability to handle high noise levels.
- **Better Noise Handling**: Incorporating noise-robust training techniques, such as dropout or denoising autoencoders, could enhance performance.

### Alternative Training Methods

- **Advanced Optimization**: Using algorithms like Adam or RMSprop may improve convergence and stability.
- **Regularization**: Adding L2 weight decay could mitigate overfitting.

### Potential Extensions

- **Other Datasets**: Applying the RBM to datasets with different types of images or tasks (e.g., handwritten text or natural images) could demonstrate its versatility.
- **Hybrid Models**: Combining the RBM with convolutional neural networks (CNNs) could leverage complementary strengths for improved performance.

## Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd rbm-neural-net-project
   ```

2. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Usage

1. Run teh RBM script:
   ```bash
   python rbm.py
   ```

2. Optional: Use command-line arguments to customize the RBM's parameters:
    ```bash
    python rbm.py --n_visible 100 --n_hidden 150 --learning_rate 0.1 --n_epochs 1000 --batch_size 4
    ```

3. View the visualizations of the original, noisy, and reconstructed images, as well as the weight heatmap.

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## Acknowledgments

- **Authors**: Special thanks to the contributors who developed and tested the RBM implementation.
- **Inspiration**: This project was inspired by foundational research on Restricted Boltzmann Machines and their applications in image processing.
- **Libraries Used**: The project leverages Python libraries such as NumPy, Matplotlib, and Scikit-learn for numerical computations, visualizations, and machine learning utilities.
- **Community**: Gratitude to the open-source community for providing resources and support.
