# Restricted Boltzmann Machine (RBM) Neural Network Project

## Overview

This project demonstrates the use of a Restricted Boltzmann Machine (RBM) to reconstruct noisy numeral images. The RBM is trained to denoise and reconstruct visual data, showcasing its ability to learn meaningful latent features.

## Purpose

The purpose of this project is to reconstruct noisy numeral images using an RBM. This approach demonstrates the RBM's ability to denoise and reconstruct visual data effectively, even in the presence of noise.

## Key Results

- Achieved a reconstruction accuracy of __________________ for noisy numeral images with a noise level of ________%.
- Reconstruction error stabilized at ______ during training, indicating effective learning.
- Visual inspection of reconstructed images showed that the RBM successfully recovered key numeral patterns, though some reconstructions were imperfect.

## Significance

This work highlights the potential of RBMs for image denoising tasks, contributing to neural network research. The results demonstrate the RBM's ability to learn meaningful latent features, making it a valuable tool for preprocessing and enhancing visual data in computer vision systems.

## Problem Description and Network Design

### Problem Statement

The task involves reconstructing noisy numeral images (digits 0-7) represented as 10x10 binary matrices. Each pixel is either active (1) or inactive (0). The primary challenges include:

- **Noise**: Distortions in the input images make it difficult to recover the original numeral patterns.
- **Limited Training Data**: Only 8 exemplars (one for each numeral) are available, requiring the model to generalize effectively.

### Network Design

#### Architecture

The Restricted Boltzmann Machine (RBM) used in this project has:

- **Visible Units**: 100 units, corresponding to the 10x10 pixel grid of the input images.
- **Hidden Units**: 150 units, which capture latent features and patterns in the data.

#### Training Methodology

The RBM is trained using the **Contrastive Divergence (CD)** algorithm, which approximates the gradient of the log-likelihood. Key training parameters include:

Key training parameters include:
- **Activation Function**: 
- **Number of Epochs**: ________ epochs to ensure convergence.
- **Learning Rate**: ______, with a decay rate of ______ to gradually reduce the step size.
- **Batch Size**: ______, allowing efficient updates while leveraging small batches of data.
- **Regularization**: 

#### Noise Handling and Reconstruction

The RBM learns robust feature representations in the hidden layer. During training, it associates noisy inputs with their corresponding clean outputs. When presented with a noisy image, the RBM reconstructs the original numeral by activating the most likely visible units based on the learned weights and biases.

## Computational Performance Analysis

### Reconstruction Error

The reconstruction error was monitored over ________ epochs. The error decreased rapidly during the initial epochs and stabilized at ________, indicating effective learning and convergence.

### Reconstruction Accuracy

The RBM's accuracy in reconstructing noisy images was evaluated using a Hamming distance threshold of ____________. The accuracy was __________, meaning the RBM correctly reconstructed ________ out of 8 numerals.

| Noise Level (%) | Reconstruction Accuracy (%) |
|------------------|-----------------------------|
| ____                | ________                      |

### Visualization

Below are examples of original, noisy, and reconstructed images:

***INSERT PICS HERE***

1. **Original Images**: Clean numeral images without noise.
2. **Noisy Images**: Input images with __________% noise.
3. **Reconstructed Images**: Outputs generated by the RBM.

### Computational Efficiency

| Parameter          | Impact on Performance                          |
|---------------------|-----------------------------------------------|
| **Hidden Units**    | Increasing hidden units improved accuracy but increased training time. |
| **Learning Rate**   | Higher rates sped up training but caused instability. |
| **Batch Size**      | Smaller batches improved generalization but slowed training. |
| **Noise Level**     | Higher noise levels reduced reconstruction accuracy. |

- **Training Time**: The RBM required approximately ________ minutes for ________ epochs.
- **Impact of Hyperparameters**:
  - Increasing the number of hidden units improved reconstruction accuracy but increased training time.
  - More epochs led to better convergence, but the reconstruction error stabilized after ________ epochs.

## Strengths and Weaknesses

### Strengths

- **Handling Moderate Noise**: The RBM effectively reconstructed images with low noise levels (________%).
- **Feature Learning**: The hidden layer captured meaningful latent features, enabling robust reconstructions.
- **Fast Convergence**: The reconstruction error stabilized quickly during training.

### Weaknesses

- **High Noise Levels**: The RBM struggled to reconstruct images with higher noise levels in earlier experiments.
- **Limited Generalization**: With only 8 training exemplars, the RBM's ability to generalize to unseen noisy inputs is limited.
- **Scalability**: The RBM's performance may degrade when applied to larger images or more complex datasets.

## Future Improvements

### Model Enhancements

- **Deeper Architectures**: Extending the RBM to a Deep Belief Network (DBN) or stacking multiple RBMs could improve its ability to handle high noise levels.
- **Better Noise Handling**: Incorporating noise-robust training techniques, such as dropout or denoising autoencoders, could enhance performance.

### Alternative Training Methods

- **Advanced Optimization**: Using algorithms like Adam or RMSprop may improve convergence and stability.
- **Regularization**: Adding L2 weight decay could mitigate overfitting.

### Potential Extensions

- **Other Datasets**: Applying the RBM to datasets with different types of images or tasks (e.g., handwritten text or natural images) could demonstrate its versatility.
- **Hybrid Models**: Combining the RBM with convolutional neural networks (CNNs) could leverage complementary strengths for improved performance.

## Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd rbm-neural-net-project
   ```

2. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Usage

1. Run teh RBM script:
   ```bash
   python rbm.py
   ```

2. Optional: Use command-line arguments to customize the RBM's parameters:
    ```bash
    python rbm.py --n_visible 100 --n_hidden 150 --learning_rate 0.1 --n_epochs 1000 --batch_size 4
    ```

3. View the visualizations of the original, noisy, and reconstructed images, as well as the weight heatmap.

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## Acknowledgments

- **Authors**: Special thanks to the contributors who developed and tested the RBM implementation.
- **Inspiration**: This project was inspired by foundational research on Restricted Boltzmann Machines and their applications in image processing.
- **Libraries Used**: The project leverages Python libraries such as NumPy, Matplotlib, and Scikit-learn for numerical computations, visualizations, and machine learning utilities.
- **Community**: Gratitude to the open-source community for providing resources and support.
