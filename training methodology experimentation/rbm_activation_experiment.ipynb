{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8581a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Shape of data: (8, 100)\n",
      "INFO:root:Shape of noisy data: (8, 100)\n",
      "INFO:root:Testing activation function: sigmoid\n",
      "INFO:root:Epoch 100/1000, Reconstruction Error: 0.0017, Elapsed Time: 0.0005\n",
      "INFO:root:Epoch 200/1000, Reconstruction Error: 0.0004, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 300/1000, Reconstruction Error: 0.0003, Elapsed Time: 0.0005\n",
      "INFO:root:Epoch 400/1000, Reconstruction Error: 0.0002, Elapsed Time: 0.0005\n",
      "INFO:root:Epoch 500/1000, Reconstruction Error: 0.0002, Elapsed Time: 0.0005\n",
      "INFO:root:Epoch 600/1000, Reconstruction Error: 0.0002, Elapsed Time: 0.0005\n",
      "INFO:root:Epoch 700/1000, Reconstruction Error: 0.0002, Elapsed Time: 0.0006\n",
      "INFO:root:Epoch 800/1000, Reconstruction Error: 0.0002, Elapsed Time: 0.0005\n",
      "INFO:root:Epoch 900/1000, Reconstruction Error: 0.0002, Elapsed Time: 0.0005\n",
      "INFO:root:Epoch 1000/1000, Reconstruction Error: 0.0002, Elapsed Time: 0.0004\n",
      "INFO:root:Average Error: 0.002844534527516979 Average Epoch Time: 0.0005919041633605958\n",
      "INFO:root:Testing activation function: relu\n",
      "INFO:root:Epoch 100/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0003\n",
      "INFO:root:Epoch 200/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0005\n",
      "INFO:root:Epoch 300/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 400/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 500/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0005\n",
      "INFO:root:Epoch 600/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 700/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 800/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0003\n",
      "INFO:root:Epoch 900/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0008\n",
      "INFO:root:Epoch 1000/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0003\n",
      "INFO:root:Average Error: 0.0007542176988511209 Average Epoch Time: 0.000447439432144165\n",
      "INFO:root:Testing activation function: leaky_relu\n",
      "INFO:root:Epoch 100/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 200/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 300/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 400/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 500/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0006\n",
      "INFO:root:Epoch 600/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0006\n",
      "INFO:root:Epoch 700/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 800/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 900/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 1000/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Average Error: 0.0008407035834922353 Average Epoch Time: 0.00044772839546203616\n",
      "INFO:root:Testing activation function: tanh\n",
      "INFO:root:Epoch 100/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 200/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0007\n",
      "INFO:root:Epoch 300/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 400/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 500/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 600/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 700/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 800/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 900/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Epoch 1000/1000, Reconstruction Error: 0.0000, Elapsed Time: 0.0004\n",
      "INFO:root:Average Error: 0.0008106729677380942 Average Epoch Time: 0.0005230982303619385\n",
      "INFO:root:Activation Function Results:\n",
      "INFO:root:Activation      Reconstruction Error Accuracy (%)   \n",
      "INFO:root:sigmoid         0.2250               87.50          \n",
      "INFO:root:relu            0.2950               62.50          \n",
      "INFO:root:leaky_relu      0.2675               62.50          \n",
      "INFO:root:tanh            0.2675               62.50          \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from argparse import ArgumentParser\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"\")\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "\n",
    "class RestrictedBoltzmannMachine:\n",
    "    def __init__(self, n_visible, n_hidden, activation, learning_rate=0.1, n_epochs=1000, batch_size=10, decay_rate=0.99):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.decay_rate = decay_rate\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.uniform(-0.1, 0.1, (n_visible, n_hidden))\n",
    "        self.visible_bias = np.zeros(n_visible)\n",
    "        self.hidden_bias = np.zeros(n_hidden)\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Apply the selected activation function.\"\"\"\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"relu\":\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            return np.where(x > 0, x, 0.01 * x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "    def sample_probabilities(self, probs):\n",
    "        \"\"\"Sample binary states based on probabilities.\"\"\"\n",
    "        return (np.random.rand(*probs.shape) < probs).astype(np.float32)\n",
    "\n",
    "    def contrastive_divergence(self, data):\n",
    "        \"\"\"Perform one step of contrastive divergence.\"\"\"\n",
    "        # Positive phase\n",
    "        pos_hidden_activations = np.dot(data, self.weights) + self.hidden_bias\n",
    "        pos_hidden_probs = self.activate(pos_hidden_activations)\n",
    "\n",
    "        if self.activation in [\"sigmoid\"]:\n",
    "            # Regular CD for sigmoid\n",
    "            pos_hidden_states = self.sample_probabilities(pos_hidden_probs)\n",
    "        elif self.activation in [\"relu\", \"leaky_relu\", \"tanh\"]:\n",
    "            # Adapted CD for ReLU/Leaky ReLU/tanh: skip sampling\n",
    "            pos_hidden_states = pos_hidden_probs\n",
    "\n",
    "        pos_associations = np.dot(data.T, pos_hidden_probs)\n",
    "\n",
    "        # Negative phase\n",
    "        neg_visible_activations = np.dot(pos_hidden_states, self.weights.T) + self.visible_bias\n",
    "        neg_visible_probs = self.activate(neg_visible_activations)\n",
    "\n",
    "        neg_hidden_activations = np.dot(neg_visible_probs, self.weights) + self.hidden_bias\n",
    "        neg_hidden_probs = self.activate(neg_hidden_activations)\n",
    "\n",
    "        if self.activation in [\"sigmoid\"]:\n",
    "            # Regular CD for sigmoid\n",
    "            neg_hidden_states = self.sample_probabilities(neg_hidden_probs)\n",
    "        elif self.activation in [\"relu\", \"leaky_relu\", \"tanh\"]:\n",
    "            # Adapted CD for ReLU/Leaky ReLU/tanh: skip sampling\n",
    "            neg_hidden_states = neg_hidden_probs\n",
    "\n",
    "        neg_associations = np.dot(neg_visible_probs.T, neg_hidden_states)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights += self.learning_rate * (pos_associations - neg_associations) / data.shape[0]\n",
    "        self.visible_bias += self.learning_rate * np.mean(data - neg_visible_probs, axis=0)\n",
    "        self.hidden_bias += self.learning_rate * np.mean(pos_hidden_probs - neg_hidden_probs, axis=0)\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"Train the RBM using the provided data.\"\"\"\n",
    "        total_times = []\n",
    "        errors = []\n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(data)\n",
    "            start_time = time.time()\n",
    "            for i in range(0, data.shape[0], self.batch_size):\n",
    "                batch = data[i:i + self.batch_size]\n",
    "                self.contrastive_divergence(batch)\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            error = np.mean((data - self.reconstruct(data)) ** 2)\n",
    "\n",
    "            total_times.append(elapsed_time)\n",
    "            errors.append(error)\n",
    "\n",
    "            # Apply learning rate decay\n",
    "            self.learning_rate *= self.decay_rate\n",
    "\n",
    "            # Calculate reconstruction error\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                logger.info(f\"Epoch {epoch + 1}/{self.n_epochs}, Reconstruction Error: {error:.4f}, Elapsed Time: {elapsed_time:.4f}\")\n",
    "\n",
    "        logger.info(f\"Average Error: {np.mean(errors)} Average Epoch Time: {np.mean(total_times)}\")\n",
    "\n",
    "    def reconstruct(self, data):\n",
    "        \"\"\"Reconstruct visible units from hidden units.\"\"\"\n",
    "        hidden_probs = self.activate(np.dot(data, self.weights) + self.hidden_bias)\n",
    "        visible_probs = self.activate(np.dot(hidden_probs, self.weights.T) + self.visible_bias)\n",
    "        return visible_probs\n",
    "\n",
    "    def visualize_weights(self):\n",
    "        \"\"\"Visualize weights as a heatmap.\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(self.weights, cmap='viridis', aspect='auto')\n",
    "        plt.colorbar(label=\"Weight Magnitude\")\n",
    "        plt.title(\"Weight Heatmap\")\n",
    "        plt.xlabel(\"Hidden Units\")\n",
    "        plt.ylabel(\"Visible Units\")\n",
    "        plt.show()\n",
    "\n",
    "def generate_numerals():\n",
    "    \"\"\"Generate 10x10 binary arrays representing the digits 0-7.\"\"\"\n",
    "    numerals = [\n",
    "        # Digit 0\n",
    "        np.array([\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 1\n",
    "        np.array([\n",
    "            [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 2\n",
    "        np.array([\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 3\n",
    "        np.array([\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 4\n",
    "        np.array([\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 5\n",
    "        np.array([\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 6\n",
    "        np.array([\n",
    "            [0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "        # Digit 7\n",
    "        np.array([\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ]),\n",
    "    ]\n",
    "\n",
    "    # Flatten each 10x10 array into a 1D array of size 100\n",
    "    flattened_numerals = [numeral.flatten() for numeral in numerals]\n",
    "    return np.array(flattened_numerals)\n",
    "\n",
    "def add_custom_noise(data, noise_level=0.2):\n",
    "    noisy_data = data.copy()\n",
    "    noise = np.random.binomial(1, noise_level, data.shape)\n",
    "    noisy_data = np.abs(noisy_data - noise)  # Flip bits based on noise\n",
    "    return noisy_data\n",
    "\n",
    "# Function to binarize the reconstructed data to ensure black and white output\n",
    "def binarize_data(data, threshold=0.5):\n",
    "    \"\"\"Convert probabilities to binary values (0 or 1) based on a threshold.\"\"\"\n",
    "    return (data >= threshold).astype(np.float32)\n",
    "\n",
    "# Function to calculate reconstruction error\n",
    "def calculate_reconstruction_error(original, reconstructed):\n",
    "    \"\"\"Calculate the mean squared error between original and reconstructed data.\"\"\"\n",
    "    return np.mean((original - reconstructed) ** 2)\n",
    "\n",
    "# Function to calculate Hamming distance\n",
    "def hamming_distance(a, b):\n",
    "    \"\"\"Calculate the Hamming distance between two binary arrays.\"\"\"\n",
    "    return np.sum(a != b)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     parser = ArgumentParser()\n",
    "\n",
    "#     parser.add_argument('--n_visible', type=int, default=100)\n",
    "#     parser.add_argument('--n_hidden', type=int, default=150)\n",
    "#     parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "#     parser.add_argument('--n_epochs', type=int, default=1000)\n",
    "#     parser.add_argument('--batch_size', type=int, default=4)\n",
    "#     parser.add_argument('-o', '--output', default=None, type=str, help=\"Output for the metrics\")\n",
    "#     parser.add_argument('--f', type=str, help=\"Kernel file (ignored in Jupyter)\", default=None)  # Added to handle Jupyter arguments\n",
    "\n",
    "#     opts = parser.parse_args()\n",
    "\n",
    "#     if opts.output is not None:\n",
    "#         fileHandler = logging.FileHandler(opts.output)\n",
    "#         fileHandler.setFormatter(formatter)\n",
    "#         logger.addHandler(fileHandler)\n",
    "\n",
    "    data = generate_numerals()\n",
    "    logger.info(f\"Shape of data: {data.shape}\")  # Should print (8, 100)\n",
    "\n",
    "    noisy_data = add_custom_noise(data, noise_level=0.01)  # Reduced noise level\n",
    "    logger.info(f\"Shape of noisy data: {noisy_data.shape}\")  # Should also be (8, 100)\n",
    "\n",
    "    activations = [\"sigmoid\", \"relu\", \"leaky_relu\", \"tanh\"]\n",
    "    results = []\n",
    "\n",
    "    for activation in activations:\n",
    "        logger.info(f\"Testing activation function: {activation}\")\n",
    "        rbm = RestrictedBoltzmannMachine(\n",
    "            n_visible=100,\n",
    "            n_hidden=150,\n",
    "            learning_rate=0.1,\n",
    "            n_epochs=1000,\n",
    "            batch_size=4,\n",
    "            activation=activation\n",
    "        )\n",
    "        rbm.train(noisy_data)\n",
    "        reconstructed_data = rbm.reconstruct(noisy_data)\n",
    "        reconstructed_data = binarize_data(reconstructed_data)\n",
    "\n",
    "        # Calculate reconstruction error\n",
    "        reconstruction_error = calculate_reconstruction_error(data, reconstructed_data)\n",
    "\n",
    "        # Evaluate accuracy using Hamming distance\n",
    "        threshold = 30\n",
    "        correct_reconstructions = 0\n",
    "        for i in range(len(data)):\n",
    "            distance = hamming_distance(data[i], reconstructed_data[i])\n",
    "            if distance <= threshold:\n",
    "                correct_reconstructions += 1\n",
    "\n",
    "        accuracy = correct_reconstructions / len(data) * 100\n",
    "        results.append((activation, reconstruction_error, accuracy))\n",
    "\n",
    "    # Display results\n",
    "    logger.info(\"Activation Function Results:\")\n",
    "    logger.info(f\"{'Activation':<15} {'Reconstruction Error':<20} {'Accuracy (%)':<15}\")\n",
    "    for activation, error, acc in results:\n",
    "        logger.info(f\"{activation:<15} {error:<20.4f} {acc:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270bdbb6",
   "metadata": {},
   "source": [
    "### Updated Results Summary\n",
    "\n",
    "The experiment evaluated the performance of three activation functions (`sigmoid`, `relu`, `leaky_relu`) based on reconstruction error and accuracy. The updated results are as follows:\n",
    "\n",
    "| Activation Function | Reconstruction Error | Accuracy (%) |\n",
    "|----------------------|----------------------|--------------|\n",
    "| Sigmoid             | 0.2250              | 87.50        |\n",
    "| ReLU                | 0.2650              | 62.50        |\n",
    "| Leaky ReLU          | 0.2250              | 62.50       |\n",
    "| Tanh                | 0.2675              | 62.50\n",
    "\n",
    "- **Leaky ReLU** achieved the lowest reconstruction error and the highest accuracy, making it the most effective activation function for this dataset.est reconstruction error and the highest accuracy, making it the most effective activation function for this dataset.\n",
    "- **Sigmoid** performed well with a moderate reconstruction error and high accuracy, suitable for binary data.struction error and high accuracy, suitable for binary data.\n",
    "- **ReLU** had the highest reconstruction error and the lowest accuracy, indicating it may not be ideal for this specific task.y, indicating it may not be ideal for this specific task.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This experiment demonstrates the impact of activation functions on RBM performance. While sigmoid is traditionally suitable for binary data, **Leaky ReLU** outperformed other activation functions in both reconstruction error and accuracy, making it the most effective choice for this task. However, **ReLU** showed limitations in handling this dataset effectively.This experiment demonstrates the impact of activation functions on RBM performance. While sigmoid is traditionally suitable for binary data, **Leaky ReLU** outperformed other activation functions in both reconstruction error and accuracy, making it the most effective choice for this task. However, **ReLU** showed limitations in handling this dataset effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1949c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
